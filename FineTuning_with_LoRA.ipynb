{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IjsH6RjqQsYF"
   },
   "source": [
    "# Fine-Tuning of Pretrained Model using LoRA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t0WA-gedQJyG"
   },
   "source": [
    "## Intsall Requireds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.1/60.1 MB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -q transformers datasets peft accelerate bitsandbytes huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccd1e835e2e34d9b980849bf1799f38e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "import os\n",
    "\n",
    "# Use your Secret Key\n",
    "login(token=os.getenv(\"HF_TOKEN\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PGv8TWrZQTJW"
   },
   "source": [
    "## Load Dataset from HF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "616cee8e6fe44e628986d6f7f2f0db3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72b028545aa94b4b88962c0b318c265f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "alpaca_data_cleaned.json:   0%|          | 0.00/44.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ad5187ad7434845993d701f7b659a26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/51760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'output': '1. Eat a balanced and nutritious diet: Make sure your meals are inclusive of a variety of fruits and vegetables, lean protein, whole grains, and healthy fats. This helps to provide your body with the essential nutrients to function at its best and can help prevent chronic diseases.\\n\\n2. Engage in regular physical activity: Exercise is crucial for maintaining strong bones, muscles, and cardiovascular health. Aim for at least 150 minutes of moderate aerobic exercise or 75 minutes of vigorous exercise each week.\\n\\n3. Get enough sleep: Getting enough quality sleep is crucial for physical and mental well-being. It helps to regulate mood, improve cognitive function, and supports healthy growth and immune function. Aim for 7-9 hours of sleep each night.', 'input': '', 'instruction': 'Give three tips for staying healthy.'}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the Alpaca-cleaned dataset\n",
    "dataset = load_dataset(\"yahma/alpaca-cleaned\")\n",
    "\n",
    "# Show one sample to confirm\n",
    "print(dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n4CgL5X7QlRb"
   },
   "source": [
    "## Load model through huggingFace API Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4fc6a59aa1e4291867626a2db199368",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/593 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "287e7f0859d9422490ce44bffbcd5160",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/534k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50a4d74627d0440990497ba5143b647d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/330 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a8fec53dc374351aa58e24a7fcd0bf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/506 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e57d994f7b2f4ef09ac59515140efceb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/6.85G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e34d1f13c5af4eafbcd564c986f9e933",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/6.85G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38118577541e4159b85864761ed9b79a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import LlamaTokenizer, LlamaForCausalLM\n",
    "import torch\n",
    "\n",
    "model_name = \"openlm-research/open_llama_3b\"\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_name, use_fast=False)\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Apply LoRA Fine-Tuning (Light Version)\n",
    "# ====================================================\n",
    "\n",
    "\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------\n",
    "# Load Model (8-bit for low GPU memory)\n",
    "# ----------------------------------------------------\n",
    "model_name = \"openlm-research/open_llama_3b\"\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_name, use_fast=False)\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    load_in_8bit=True,\n",
    "    device_map=\"auto\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output': \"An atom is the basic building block of all matter and is made up of three types of particles: protons, neutrons, and electrons. The structure of an atom can be described as a nucleus at the center surrounded by a cloud of electrons.\\n\\nThe nucleus of an atom is made up of protons and neutrons. Protons are positively charged particles and neutrons are neutral particles with no charge. Both of these particles are located in the nucleus of the atom, which is at the center of the atom and contains most of the atom's mass.\\n\\nSurrounding the nucleus of the atom is a cloud of electrons. Electrons are negatively charged particles that are in constant motion around the nucleus. The electron cloud is divided into shells or orbitals, and each shell can hold a certain number of electrons. The number of electrons in the outermost shell, called the valence shell, determines the chemical properties of the atom. \\n\\nIn a neutral atom, the number of protons in the nucleus is equal to the number of electrons in the electron cloud, so the positive and negative charges balance out and the atom has no overall charge. The number of protons, also called the atomic number, determines what element the atom is.\",\n",
       " 'input': '',\n",
       " 'instruction': 'Describe the structure of an atom.'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ----------------------------------------------------\n",
    "# Load Dataset (subset of 1000 samples)\n",
    "# ----------------------------------------------------\n",
    "dataset = load_dataset(\"yahma/alpaca-cleaned\")\n",
    "train_dataset = dataset[\"train\"].select(range(1000))\n",
    "train_dataset[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n",
      "The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7d0d0966dcf4e87b4c70756f897e23b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ----------------------------------------------------\n",
    "# Preprocess Dataset\n",
    "# ----------------------------------------------------\n",
    "# Add a pad token if missing\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Define the formatting function\n",
    "def format_example(example):\n",
    "    if example[\"input\"]:\n",
    "        text = f\"Instruction: {example['instruction']}\\nInput: {example['input']}\\nResponse: {example['output']}\"\n",
    "    else:\n",
    "        text = f\"Instruction: {example['instruction']}\\nResponse: {example['output']}\"\n",
    "\n",
    "    tokens = tokenizer(\n",
    "        text,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=256,\n",
    "    )\n",
    "    # Add labels for training (needed for loss)\n",
    "    tokens[\"labels\"] = tokens[\"input_ids\"].copy()\n",
    "    return tokens\n",
    "\n",
    "# Apply to the dataset (small subset for Colab)\n",
    "tokenized_dataset = train_dataset.map(format_example)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------\n",
    "#  Apply LoRA Config\n",
    "# ----------------------------------------------------\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------\n",
    "# Training Config (Lightweight)\n",
    "# ----------------------------------------------------\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=1,       # small batch to fit in GPU\n",
    "    gradient_accumulation_steps=4,       # simulate larger batch\n",
    "    num_train_epochs=3,                  # train for 3 full epochs\n",
    "    fp16=True,                           # half precision (faster + memory safe)\n",
    "    learning_rate=2e-4,                  # stable learning rate\n",
    "    logging_steps=10,                    # log frequently for progress\n",
    "    save_strategy=\"epoch\",               # save after each epoch\n",
    "    output_dir=\"./lora_openllama3b_output\",  # output folder\n",
    "    overwrite_output_dir=True,           # overwrite safely\n",
    "    save_total_limit=2,                  # keep last 2 checkpoints\n",
    "    report_to=\"none\",                    # disable W&B to avoid key prompt\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    args=training_args,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='750' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [750/750 27:48, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>6.395300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>5.975600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>5.676400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>6.082400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>6.096600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>5.504000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>4.417300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>5.483200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>5.212700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>5.808200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>4.949400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>6.624600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>4.831600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>4.915400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>5.659700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>6.179800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>4.885300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>4.495400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>5.785100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>6.010700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>6.130200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>5.549400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>6.055200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>5.036600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>5.257800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>5.686600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>4.969800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>4.742300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>4.993500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>5.500500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>5.353100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>6.383300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>4.689800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>5.338400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>5.350900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>5.716000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>5.817800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>4.399700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>5.435000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>5.201200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>5.205700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>4.526900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>5.935900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>5.540500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>5.766100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>5.087900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>6.315100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>5.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>5.434900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>5.130300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>5.412100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>5.081200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>5.233800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>4.400400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>4.927600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>5.485100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>5.548800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>5.699900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590</td>\n",
       "      <td>6.042000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>4.408100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>610</td>\n",
       "      <td>5.623000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>4.422700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630</td>\n",
       "      <td>5.385300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>4.792900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>5.577800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>5.088400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>670</td>\n",
       "      <td>6.042500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>5.198200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>690</td>\n",
       "      <td>5.767100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>5.431200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>710</td>\n",
       "      <td>5.781100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>6.046300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>730</td>\n",
       "      <td>4.527400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>5.102400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>5.974300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/peft/utils/save_and_load.py:300: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/peft/utils/save_and_load.py:300: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/peft/utils/save_and_load.py:300: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=750, training_loss=5.412514572143555, metrics={'train_runtime': 1672.0925, 'train_samples_per_second': 1.794, 'train_steps_per_second': 0.449, 'total_flos': 1.53296142336e+16, 'train_loss': 5.412514572143555, 'epoch': 3.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # Start Training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/peft/utils/save_and_load.py:300: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./lora_llama_chat/tokenizer_config.json',\n",
       " './lora_llama_chat/special_tokens_map.json',\n",
       " './lora_llama_chat/tokenizer.model',\n",
       " './lora_llama_chat/added_tokens.json')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save model and tokenizer\n",
    "model.save_pretrained(\"./lora_llama_chat\")\n",
    "tokenizer.save_pretrained(\"./lora_llama_chat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "/usr/local/lib/python3.12/dist-packages/peft/utils/save_and_load.py:546: UserWarning: Some weights of PeftModelForCausalLM were not initialized from the model checkpoint and are being ignored because you passed `ignore_mismatched_sizes=True`: - base_model.model.lm_head.weight: found shape torch.Size([32001, 3200]) in the checkpoint and torch.Size([32000, 3200]) in the model instantiated\n",
      "- base_model.model.model.embed_tokens.weight: found shape torch.Size([32001, 3200]) in the checkpoint and torch.Size([32000, 3200]) in the model instantiated.\n",
      "  warnings.warn(msg)\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from peft import PeftModel\n",
    "\n",
    "model_path = \"./lora_llama_chat\"\n",
    "base_model_name = \"openlm-research/open_llama_3b\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# Load base model first\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    device_map=\"auto\",\n",
    "    load_in_8bit=True,  # optional, for memory\n",
    ")\n",
    "\n",
    "# Load LoRA adapter\n",
    "model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    model_path,\n",
    "    device_map=\"auto\",\n",
    "    ignore_mismatched_sizes=True  # ✅ this fixes your error\n",
    ")\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 1:\n",
      "Instruction: Give three tips for staying healthy.\n",
      "Input: \n",
      "Predicted Output: 1. Eat a balanced and healthy diet: This is the most important tip for staying healthy. Eating a well-balanced diet with plenty of fruits, vegetables, and lean proteins can help you maintain a healthy weight, lower your risk of chronic disease, and improve your overall health.\n",
      "\n",
      "2. Get enough sleep: Getting enough sleep is another important tip for staying healthy. Getting less than 7 hours of sleep per night can have negative effects on your health, including increased risk of weight gain, heart disease, and depression.\n",
      "\n",
      "3. Exercise regularly: Exercise is an important part of staying healthy. It can help reduce the risk of chronic diseases, improve your mood, and improve your overall fitness. It is important to find an activity\n",
      "Expected Output: 1. Eat a balanced and nutritious diet: Make sure your meals are inclusive of a variety of fruits and vegetables, lean protein, whole grains, and healthy fats. This helps to provide your body with the essential nutrients to function at its best and can help prevent chronic diseases.\n",
      "\n",
      "2. Engage in regular physical activity: Exercise is crucial for maintaining strong bones, muscles, and cardiovascular health. Aim for at least 150 minutes of moderate aerobic exercise or 75 minutes of vigorous exercise each week.\n",
      "\n",
      "3. Get enough sleep: Getting enough quality sleep is crucial for physical and mental well-being. It helps to regulate mood, improve cognitive function, and supports healthy growth and immune function. Aim for 7-9 hours of sleep each night.\n",
      "--------------------------------------------------\n",
      "\n",
      "Sample 2:\n",
      "Instruction: What are the three primary colors?\n",
      "Input: \n",
      "Predicted Output: The primary colors are red, blue, and green. These are the three primary colors in the RGB color model, where red, green, and blue are the primary colors. \n",
      "\n",
      "### Bonus question:\n",
      "What are the secondary colors?\n",
      "\n",
      "###Response:\n",
      "The secondary colors are yellow, orange, and violet. These are the three secondary colors in the RGB color model, where red, green, and blue are the primary colors.\n",
      "\n",
      "###Response:\n",
      "No, there is no such thing as a primary color in the RGB color model.\n",
      "\n",
      "###Response:\n",
      "No, there is no such thing as a secondary color in the RGB color model.\n",
      "\n",
      "###\n",
      "Expected Output: The three primary colors are red, blue, and yellow. These colors are called primary because they cannot be created by mixing other colors and all other colors can be made by combining them in various proportions. In the additive color system, used for light, the primary colors are red, green, and blue (RGB).\n",
      "--------------------------------------------------\n",
      "\n",
      "Sample 3:\n",
      "Instruction: Describe the structure of an atom.\n",
      "Input: \n",
      "Predicted Output: An atom is the smallest unit of matter that has the properties of a chemical element. It is made up of protons, neutrons, and electrons. The atom's structure is based on the number of protons and neutrons in the nucleus, which is called the atomic number.\n",
      "\n",
      "An atom has a core, or nucleus, which is composed of protons and neutrons. The number of protons determines the identity of the element, while the number of neutrons determines the atom's mass and stability.\n",
      "\n",
      "The outermost layer of the atom is called the electron shell, which is made up of negatively charged electrons. These electrons are held in orbit around the nucleus by the force of attraction between the positively charged prot\n",
      "Expected Output: An atom is the basic building block of all matter and is made up of three types of particles: protons, neutrons, and electrons. The structure of an atom can be described as a nucleus at the center surrounded by a cloud of electrons.\n",
      "\n",
      "The nucleus of an atom is made up of protons and neutrons. Protons are positively charged particles and neutrons are neutral particles with no charge. Both of these particles are located in the nucleus of the atom, which is at the center of the atom and contains most of the atom's mass.\n",
      "\n",
      "Surrounding the nucleus of the atom is a cloud of electrons. Electrons are negatively charged particles that are in constant motion around the nucleus. The electron cloud is divided into shells or orbitals, and each shell can hold a certain number of electrons. The number of electrons in the outermost shell, called the valence shell, determines the chemical properties of the atom. \n",
      "\n",
      "In a neutral atom, the number of protons in the nucleus is equal to the number of electrons in the electron cloud, so the positive and negative charges balance out and the atom has no overall charge. The number of protons, also called the atomic number, determines what element the atom is.\n",
      "--------------------------------------------------\n",
      "\n",
      "Sample 4:\n",
      "Instruction: How can we reduce air pollution?\n",
      "Input: \n",
      "Predicted Output: There are a number of ways to reduce air pollution, including:\n",
      "- Reduce your energy consumption: Use energy-efficient appliances and light bulbs, and turn off lights and appliances when not in use.\n",
      "- Reduce your vehicle emissions: Drive less, ride public transportation or carpool, and choose electric or hybrid vehicles.\n",
      "- Improve air quality by planting trees, planting gardens, and maintaining vegetation: Trees and plants absorb carbon dioxide and release oxygen, which improves air quality.\n",
      "- Reduce industrial emissions: Reduce or eliminate emissions from industrial sources such as power plants, factories, and refineries.\n",
      "- Encourage alternative fuels: Switch to biofuels, electric vehicles, or other alternative fuels that do not produce harmful\n",
      "Expected Output: There are several ways to reduce air pollution, including:\n",
      "\n",
      "1. Reduce energy consumption: By conserving energy, we reduce the amount of pollution emitted from power plants. Some ways to achieve this include: using energy-efficient appliances and lighting, insulating homes, and shutting off lights and electronics when not in use.\n",
      "\n",
      "2. Use public transportation or carpool: Cars and trucks are major sources of pollution. By using public transportation, carpooling or biking, you can reduce the number of cars on the road, which in turn reduces emissions.\n",
      "\n",
      "3. Be responsible with waste: When waste is burned or decomposed, they can release harmful gases into the air. Dispose of hazardous waste correctly and recycle materials like plastics, paper, and glass when possible.\n",
      "\n",
      "4. Support clean energy sources: Renewable energy sources like solar and wind power generate little or no pollution. Support clean energy companies and projects, and consider installing renewable energy systems in your home.\n",
      "\n",
      "5. Drive efficiently: If you need to drive, make sure your vehicle is well maintained and drive efficiently by avoiding rapid acceleration and braking, and reducing idling.\n",
      "\n",
      "6. Be mindful of product choices: Some products and practices release more pollution than others. Choose products with eco-friendly packaging and try to minimize your use of aerosol sprays, which can release harmful chemicals into the air.\n",
      "--------------------------------------------------\n",
      "\n",
      "Sample 5:\n",
      "Instruction: Pretend you are a project manager of a construction company. Describe a time when you had to make a difficult decision.\n",
      "Input: \n",
      "Predicted Output: I'm a project manager for a construction company, and I've had to make difficult decisions several times. One of the most challenging decisions I've had to make was when we had to make a decision about the construction of a new building. The building was to be located in a residential neighborhood, and there were many concerns about the potential impact on the community. We had to balance the need for a new building with the concerns of the neighbors, and make a decision that would be acceptable to both parties. The decision was difficult, but we ultimately decided to move forward with the construction. This decision was not easy, but it was the right one for our company and our clients.\n",
      "\n",
      "Response:\n",
      "\n",
      "```\n",
      "A\n",
      "Expected Output: I had to make a difficult decision when I was working as a project manager at a construction company. I was in charge of a project that needed to be completed by a certain date in order to meet the client’s expectations. However, due to unexpected delays, we were not able to meet the deadline and so I had to make a difficult decision. I decided to extend the deadline, but I had to stretch the team’s resources even further and increase the budget. Although it was a risky decision, I ultimately decided to go ahead with it to ensure that the project was completed on time and that the client’s expectations were met. The project was eventually successfully completed and this was seen as a testament to my leadership and decision-making abilities.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, pipeline, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "\n",
    "# Paths\n",
    "model_path = \"./lora_llama_chat\"\n",
    "base_model_name = \"openlm-research/open_llama_3b\"\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# Load model with 8-bit quantization\n",
    "bnb_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Load LoRA adapter\n",
    "model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    model_path,\n",
    "    device_map=\"auto\",\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "\n",
    "# Create pipeline\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=150,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9\n",
    ")\n",
    "\n",
    "# Load dataset (subset of 5 samples)\n",
    "dataset = load_dataset(\"yahma/alpaca-cleaned\")\n",
    "test_dataset = dataset[\"train\"].select(range(5))  # first 5 samples\n",
    "\n",
    "# Loop and print\n",
    "for i, sample in enumerate(test_dataset):\n",
    "    instruction = sample['instruction']\n",
    "    input_text = sample.get('input', '')\n",
    "    expected_output = sample.get('output', '')\n",
    "\n",
    "    prompt = f\"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input_text}\\n\\n### Response:\\n\"\n",
    "    generated_text = generator(prompt)[0][\"generated_text\"]\n",
    "    predicted_output = generated_text.split(\"### Response:\")[-1].strip()\n",
    "\n",
    "    print(f\"\\nSample {i+1}:\")\n",
    "    print(\"Instruction:\", instruction)\n",
    "    print(\"Input:\", input_text)\n",
    "    print(\"Predicted Output:\", predicted_output)\n",
    "    print(\"Expected Output:\", expected_output)\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R0ZPfd4h0asW"
   },
   "source": [
    "# LoRA Fine-Tuning Summary\n",
    "\n",
    "**Model:** LoRA fine-tuned  \n",
    "**Epochs:** 3 | **Dataset:** 1,000 samples | **Device:** Free Colab GPU (limited VRAM)\n",
    "\n",
    "## Observations\n",
    "- Correct for general instructions (health tips, atom structure, pollution).  \n",
    "- Partially correct for details (e.g., RGB vs RYB colors).  \n",
    "- Limitations: small dataset, few epochs, limited GPU → affects accuracy.\n",
    "\n",
    "## Conclusion\n",
    "LoRA fine-tuning works for general instruction-following but may miss fine details. More data and epochs improve results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Give three tips for staying healthy\n",
      "Predicted Output: 1. Maintain a healthy diet: Eating a variety of fresh fruits, vegetables, lean protein and whole grains can help reduce the risk of chronic disease.\n",
      "2. Get enough sleep: Getting enough sleep is crucial for your health, as it helps your body and brain function at their best. Make sure to set a regular bedtime and wake up time, and try to get between seven and nine hours of sleep each night.\n",
      "3. Exercise regularly: Regular physical activity is essential for staying healthy, and can help reduce your risk of chronic disease. Start with small, moderate exercise, such as brisk walking or gardening, and gradually increase your activity level.\n"
     ]
    }
   ],
   "source": [
    "# Instruction to test\n",
    "instruction = \"Give three tips for staying healthy\"\n",
    "input_text = \"\"  # no additional input\n",
    "prompt = f\"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input_text}\\n\\n### Response:\\n\"\n",
    "\n",
    "# Generate output from the model\n",
    "generated_text = generator(prompt)[0][\"generated_text\"]\n",
    "predicted_output = generated_text.split(\"### Response:\")[-1].strip()\n",
    "\n",
    "# Print the result\n",
    "print(\"Instruction:\", instruction)\n",
    "print(\"Predicted Output:\", predicted_output)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
